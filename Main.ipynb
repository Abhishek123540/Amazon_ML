{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84353c94-4853-4c6c-9bee-ec07b2cc36fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MultiCPU code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf49989-74a0-4e19-a570-7582cf9d5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def preprocess_image(image_path, output_path=None):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray_img = img.convert('L')\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(gray_img)\n",
    "    \n",
    "    # Normalize pixel values to range [0, 1]\n",
    "    normalized_img = img_array / 255.0\n",
    "    \n",
    "    # Optional: Apply additional normalization (e.g., zero mean and unit variance)\n",
    "    mean = np.mean(normalized_img)\n",
    "    std = np.std(normalized_img)\n",
    "    standardized_img = (normalized_img - mean) / std\n",
    "    \n",
    "    # Convert back to PIL Image\n",
    "    preprocessed_img = Image.fromarray((standardized_img * 255).astype(np.uint8))\n",
    "    \n",
    "    # Save the preprocessed image if output_path is provided\n",
    "    if output_path:\n",
    "        preprocessed_img.save(output_path)\n",
    "    \n",
    "    return preprocessed_img\n",
    "\n",
    "def process_single_image(args):\n",
    "    input_image_path, output_image_path = args\n",
    "    # Preprocess the image\n",
    "    preprocess_image(input_image_path, output_image_path)\n",
    "    print(f\"Processed {os.path.basename(input_image_path)} and saved to {output_image_path}\")\n",
    "\n",
    "def preprocess_images_in_folder(input_folder, output_folder, max_workers=4):\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Create a list of image paths\n",
    "    image_paths = [\n",
    "        (os.path.join(input_folder, filename), os.path.join(output_folder, filename))\n",
    "        for filename in os.listdir(input_folder)\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n",
    "    ]\n",
    "    \n",
    "    # Use ThreadPoolExecutor to parallelize the image preprocessing\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        executor.map(process_single_image, image_paths)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"C:\\Users\\Abhishek\\Desktop\\Amazon_ML\\train_images\\all_images\"\n",
    "    output_folder = r\"C:\\Users\\Abhishek\\Desktop\\Amazon_ML\\processed_train_images\"\n",
    "    \n",
    "    # Preprocess all images in the folder using multiple CPU threads\n",
    "    preprocess_images_in_folder(input_folder, output_folder, max_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01220018-13ab-4a83-8e49-6ad14a9e7dee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GreyScale Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c4125-8ded-47f5-aa10-8f4c8902355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_folder = \"C:/Users/Abhishek/Desktop/Amazon_ML/train_images/all_images\"\n",
    "output_folder = \"C:/Users/Abhishek/Desktop/Amazon_ML/grey_train_images\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(filename):\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    \n",
    "    if os.path.isfile(img_path):\n",
    "        try:\n",
    "            # Load the image\n",
    "            image = cv2.imread(img_path)\n",
    "            \n",
    "            if image is None:\n",
    "                print(f\"Warning: Image {filename} could not be loaded.\")\n",
    "                return\n",
    "\n",
    "            # Convert to grayscale\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Normalize the image\n",
    "            normalized_image = gray_image / 255.0\n",
    "            \n",
    "            # Resize the image\n",
    "            resized_image = cv2.resize(normalized_image, (256, 256))\n",
    "            \n",
    "            # Save the preprocessed image\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, (resized_image * 255).astype(np.uint8))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Process images with multiple threads\n",
    "def process_images_in_parallel(image_filenames):\n",
    "    with Pool(processes=8) as pool:\n",
    "        for _ in tqdm(pool.imap_unordered(preprocess_image, image_filenames), total=len(image_filenames)):\n",
    "            pass\n",
    "\n",
    "def batch_process_images(batch_size=1000):\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    all_filenames = [f for f in os.listdir(input_folder) if os.path.splitext(f)[1].lower() in valid_extensions]\n",
    "    \n",
    "    for i in range(0, len(all_filenames), batch_size):\n",
    "        batch_filenames = all_filenames[i:i + batch_size]\n",
    "        process_images_in_parallel(batch_filenames)\n",
    "        print(f\"Processed batch {i // batch_size + 1}\")\n",
    "\n",
    "# Start the processing\n",
    "if __name__ == '__main__':\n",
    "    batch_process_images()\n",
    "    print(f\"Preprocessing complete. Processed images saved in {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87194e9-d973-4e45-a74f-adb9dc419ec5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feature Extraction on RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8927a-e2e1-4179-a6f4-297aab6db247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load pre-trained ResNet50 model without the top classification layer\n",
    "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "def preprocess_image(image_path, output_path=None):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Resize the image to 224x224 (ResNet input size)\n",
    "    img = img.resize((224, 224))\n",
    "    \n",
    "    # Convert image to numpy array\n",
    "    img_array = img_to_array(img)\n",
    "    \n",
    "    # Preprocess the image for ResNet50\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def extract_features(image_path):\n",
    "    # Preprocess the image\n",
    "    img_array = preprocess_image(image_path)\n",
    "    \n",
    "    # Expand dimensions to match model input\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Extract features using ResNet50\n",
    "    features = model.predict(img_array)\n",
    "    \n",
    "    return features.flatten()\n",
    "\n",
    "def process_single_image(args):\n",
    "    input_image_path, output_feature_path = args\n",
    "    # Extract features from the image\n",
    "    features = extract_features(input_image_path)\n",
    "    \n",
    "    # Save features as a .npy file\n",
    "    np.save(output_feature_path, features)\n",
    "    print(f\"Extracted features from {os.path.basename(input_image_path)} and saved to {output_feature_path}\")\n",
    "\n",
    "def extract_features_from_images(input_folder, output_folder, max_workers=4):\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Create a list of image paths\n",
    "    image_paths = [\n",
    "        (os.path.join(input_folder, filename), os.path.join(output_folder, filename.split('.')[0] + '.npy'))\n",
    "        for filename in os.listdir(input_folder)\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n",
    "    ]\n",
    "    \n",
    "    # Use ThreadPoolExecutor to parallelize the feature extraction\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        executor.map(process_single_image, image_paths)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"C:\\Users\\Abhishek\\Desktop\\Amazon_ML\\train_images\\all_images\"\n",
    "    output_folder = r\"C:\\Users\\Abhishek\\Desktop\\Amazon_ML\\extracted_features\"\n",
    "    \n",
    "    # Extract features from all images in the folder using multiple CPU threads\n",
    "    extract_features_from_images(input_folder, output_folder, max_workers=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518a557-c7fb-4d6d-bc6e-c561aa81f19b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61dc1fb5-a525-45aa-8ebc-29a51de83003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must provide at least one structure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m\n\u001b[0;32m     28\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Abhishek/Desktop/Amazon_ML/output\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\src\\tree\\optree_impl.py:76\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`func` must be callable. Received: func=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m structures:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide at least one structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     78\u001b[0m     assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Must provide at least one structure"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define your input shape and number of classes\n",
    "input_shape = (256, 256, 1)  # Grayscale images of size 256x256\n",
    "num_classes = 10  # Assuming you have 10 classes\n",
    "\n",
    "# Create an instance of your model\n",
    "model = create_resnet_model(input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "# Compile the model (define optimizer, loss function, and metrics)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load your dataset (example with ImageDataGenerator)\n",
    "# Ensure your images are grayscale and resized to (256, 256)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'C:/Users/Abhishek/Desktop/Amazon_ML/output',  # Replace with the path to your dataset\n",
    "    target_size=(256, 256),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'C:/Users/Abhishek/Desktop/Amazon_ML/output',\n",
    "    target_size=(256, 256),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('resnet_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be044333-daad-405d-9b5c-0fa5c97d5bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>entity_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n",
       "      <td>748919</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>500.0 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n",
       "      <td>916768</td>\n",
       "      <td>item_volume</td>\n",
       "      <td>1.0 cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n",
       "      <td>459516</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.709 gram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_link  group_id  entity_name  \\\n",
       "0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n",
       "1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n",
       "2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n",
       "\n",
       "  entity_value  \n",
       "0   500.0 gram  \n",
       "1      1.0 cup  \n",
       "2   0.709 gram  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('dataset/train.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e317d58-6f03-41bc-8c6e-4a18ae8e9f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         item_weight\n",
       "1         item_volume\n",
       "2         item_weight\n",
       "3         item_weight\n",
       "4         item_weight\n",
       "             ...     \n",
       "263854         height\n",
       "263855         height\n",
       "263856         height\n",
       "263857         height\n",
       "263858         height\n",
       "Name: entity_name, Length: 263859, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df['entity_name']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d244d1e-24f0-4ea5-8ef1-3ca4c7b06b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30241e57-04e9-4fdb-9d67-4a5c07f288d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['entity_name'] = label_encoder.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b84606-3883-4ced-afe0-cdafaf71f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         3\n",
      "1         2\n",
      "2         3\n",
      "3         3\n",
      "4         3\n",
      "         ..\n",
      "263854    1\n",
      "263855    1\n",
      "263856    1\n",
      "263857    1\n",
      "263858    1\n",
      "Name: entity_name, Length: 263859, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(df['entity_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a0c90-79e6-4900-9d05-b0faba941077",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "efc4d282-15a1-4005-90af-ec9ffbaa4bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    ... 4.535 0.    0.   ]\n",
      " [0.    0.    0.    ... 5.21  0.    0.   ]\n",
      " [0.    0.    0.    ... 3.758 0.    0.   ]\n",
      " ...\n",
      " [0.    0.    0.    ... 4.387 0.    0.   ]\n",
      " [0.    0.    0.    ... 4.54  0.    0.   ]\n",
      " [0.    0.    0.    ... 4.32  0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to hold the rows\n",
    "all_arrays = []\n",
    "\n",
    "# Loop through numbers 0 to 111\n",
    "for a in range(0, 112):\n",
    "    file_path = f'output/image_{a}.npy'\n",
    "\n",
    "    # Load the .npy file\n",
    "    data = np.load(file_path)\n",
    "    \n",
    "    # Append the 1D array to the list\n",
    "    all_arrays.append(data)\n",
    "\n",
    "# Convert the list of arrays into a 2D NumPy array\n",
    "x = np.vstack(all_arrays)\n",
    "\n",
    "# Print the resulting 2D array\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51dfe560-4ce0-4dda-87b0-a467f8a7d550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100352\n"
     ]
    }
   ],
   "source": [
    "length = len(data)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53af34e8-fe48-4483-a047-12663a7af448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            image_link  group_id  entity_name  \\\n",
      "0    https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n",
      "1    https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n",
      "2    https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n",
      "3    https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n",
      "4    https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n",
      "..                                                 ...       ...          ...   \n",
      "107  https://m.media-amazon.com/images/I/61xxqfM2Ew...    991868  item_weight   \n",
      "108  https://m.media-amazon.com/images/I/81yG9eUKvx...    208023  item_weight   \n",
      "109  https://m.media-amazon.com/images/I/61FMOl299l...    593600  item_weight   \n",
      "110  https://m.media-amazon.com/images/I/41Kn+YOyPj...    459516  item_volume   \n",
      "111  https://m.media-amazon.com/images/I/51q9OE6hfg...    459516  item_volume   \n",
      "\n",
      "         entity_value  \n",
      "0          500.0 gram  \n",
      "1             1.0 cup  \n",
      "2          0.709 gram  \n",
      "3          0.709 gram  \n",
      "4      1400 milligram  \n",
      "..                ...  \n",
      "107      9.0 kilogram  \n",
      "108      2.5 kilogram  \n",
      "109         50.0 gram  \n",
      "110   10.0 millilitre  \n",
      "111  200.0 millilitre  \n",
      "\n",
      "[112 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example data to save\n",
    "df=pd.read_csv('dataset/train.csv')  # A 5x3 array of random numbers\n",
    "\n",
    "# Convert the NumPy array to a DataFrame\n",
    "df = df.iloc[:112, :]\n",
    "print(df)\n",
    "# Save the DataFrame to a CSV file\n",
    "# df.to_csv('output/data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea91118d-f82f-448d-b0d7-750c19ddccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            image_data  group_id  entity_name  \\\n",
      "0    https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n",
      "1    https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n",
      "2    https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n",
      "3    https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n",
      "4    https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n",
      "..                                                 ...       ...          ...   \n",
      "107  https://m.media-amazon.com/images/I/61xxqfM2Ew...    991868  item_weight   \n",
      "108  https://m.media-amazon.com/images/I/81yG9eUKvx...    208023  item_weight   \n",
      "109  https://m.media-amazon.com/images/I/61FMOl299l...    593600  item_weight   \n",
      "110  https://m.media-amazon.com/images/I/41Kn+YOyPj...    459516  item_volume   \n",
      "111  https://m.media-amazon.com/images/I/51q9OE6hfg...    459516  item_volume   \n",
      "\n",
      "         entity_value  \n",
      "0          500.0 gram  \n",
      "1             1.0 cup  \n",
      "2          0.709 gram  \n",
      "3          0.709 gram  \n",
      "4      1400 milligram  \n",
      "..                ...  \n",
      "107      9.0 kilogram  \n",
      "108      2.5 kilogram  \n",
      "109         50.0 gram  \n",
      "110   10.0 millilitre  \n",
      "111  200.0 millilitre  \n",
      "\n",
      "[112 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={'image_link': 'image_data'}, inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94eedd7d-7197-40a2-885f-6d6ffb2e2c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     group_id  entity_name      entity_value\n",
      "0      748919  item_weight        500.0 gram\n",
      "1      916768  item_volume           1.0 cup\n",
      "2      459516  item_weight        0.709 gram\n",
      "3      459516  item_weight        0.709 gram\n",
      "4      731432  item_weight    1400 milligram\n",
      "..        ...          ...               ...\n",
      "107    991868  item_weight      9.0 kilogram\n",
      "108    208023  item_weight      2.5 kilogram\n",
      "109    593600  item_weight         50.0 gram\n",
      "110    459516  item_volume   10.0 millilitre\n",
      "111    459516  item_volume  200.0 millilitre\n",
      "\n",
      "[112 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "y=df[['group_id','entity_name','entity_value']]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d54f21c7-8e71-47b6-94c9-f1c113d4556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     group_id  entity_name      entity_value\n",
      "0      748919  item_weight        500.0 gram\n",
      "1      916768  item_volume           1.0 cup\n",
      "2      459516  item_weight        0.709 gram\n",
      "3      459516  item_weight        0.709 gram\n",
      "4      731432  item_weight    1400 milligram\n",
      "..        ...          ...               ...\n",
      "107    991868  item_weight      9.0 kilogram\n",
      "108    208023  item_weight      2.5 kilogram\n",
      "109    593600  item_weight         50.0 gram\n",
      "110    459516  item_volume   10.0 millilitre\n",
      "111    459516  item_volume  200.0 millilitre\n",
      "\n",
      "[112 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example data\n",
    "# data = np.array([['A'], ['B'], ['A'], ['C']])\n",
    "\n",
    "# Create OneHotEncoder instance\n",
    "encoder = OneHotEncoder()\n",
    "df=pd.DataFrame(y)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "09a78776-6f83-4ace-af17-1e02e4c60e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 2 1 1 1 1 1 1 1 1 1 1 3\n",
      " 2 1 1 1 1 3 3 3 3 1 1 3 0 1 1 1 1 1 1 1 1 1 1 1 1 3 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 3 1 1 3 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
      " 0]\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "vals = le.fit_transform(df['entity_name'])\n",
    "\n",
    "print(\"Encoded labels:\", vals)\n",
    "print(len(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8379bbec-b845-4097-a077-4da7a949d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8c1dd265-1f74-4f19-94f5-5703f6ac6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size = 0.05,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "79f5b46f-30e8-4077-9edb-dd6f68356dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    ... 4.375 0.    0.   ]\n",
      " [0.    0.    0.    ... 6.027 0.    0.   ]\n",
      " [0.    0.    0.    ... 5.07  0.    0.   ]\n",
      " ...\n",
      " [0.    0.    0.    ... 4.53  0.    0.   ]\n",
      " [0.    0.    0.    ... 4.996 0.    0.   ]\n",
      " [0.    0.    0.    ... 4.38  0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eb937a9c-3a63-4a98-9560-3c3daf12fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fbadeba3-ac86-417c-9641-30d76c3ca49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 528ms/step - accuracy: 0.1063 - loss: 16.8584 - val_accuracy: 0.1250 - val_loss: 79.9239\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step - accuracy: 0.1141 - loss: 81.3689 - val_accuracy: 0.0000e+00 - val_loss: 36.5894\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 441ms/step - accuracy: 0.0907 - loss: 36.3230 - val_accuracy: 0.1250 - val_loss: 52.8579\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442ms/step - accuracy: 0.0326 - loss: 53.2074 - val_accuracy: 0.2500 - val_loss: 24.2229\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step - accuracy: 0.1506 - loss: 24.6958 - val_accuracy: 0.1250 - val_loss: 39.9213\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430ms/step - accuracy: 0.2478 - loss: 25.0381 - val_accuracy: 0.0000e+00 - val_loss: 39.5184\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442ms/step - accuracy: 0.1437 - loss: 22.4362 - val_accuracy: 0.1250 - val_loss: 22.8920\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431ms/step - accuracy: 0.1901 - loss: 19.2291 - val_accuracy: 0.2500 - val_loss: 29.3831\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 433ms/step - accuracy: 0.2726 - loss: 24.2831 - val_accuracy: 0.0000e+00 - val_loss: 19.9619\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456ms/step - accuracy: 0.0404 - loss: 18.1381 - val_accuracy: 0.1250 - val_loss: 16.0760\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1500 - loss: 19.8205\n",
      "Test loss: 19.8205\n",
      "Test accuracy: 0.1500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Predictions shape: (20, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Build the fully connected model\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(100352,)),  # Adjust input_shape to match feature vector length\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Number of classes in the output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(xtrain, ytrain, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(xtest, ytest)\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(xtest)\n",
    "print(f'Predictions shape: {predictions.shape}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
